# CrowdData

CrowdData is an open repository that aggregates the crowdsourced datasets that have individual crowd votes. We aim at providing the available datasets with a standard format (explained in `Download` section below) so that they can be directly used in experiments, without any work-load in preprocessing. Datasets included in this repo serve for classification tasks (mainly text classification, except Emotion Dataset). CrowData can benefit researchers investigating hybrid usage of machine and human-in-the-loop in classification tasks (the repo includes 5 datasets having the actual content of the tasks), human in classification and ranking tasks, truth discovery based on crowdsourced data, estimation of the crowd bias, and active learning. **If you use any of the datasets in this repository, please make sure that you've read and followed the usage consent we explain at the bottom of this page.**

## Datasets

We categorized the datasets in two folders: `binary-classification` and `multi-class-classification`. Within each folder, each dataset is kept in a separate folder having a link to the original source. Table below shows an overview of the datasets. The columns of the table are as follows:

* `Dataset`: Name of the dataset including a link to the original source.
* `Description`: A brief description of the dataset.
* `Number of tasks`: The number of tasks asked to the crowd.
* `Number of workers`: Number of crowd workers completing the tasks.
* `Number of total votes`: Number of votes collected for all tasks.
* `Ground Truth`: Are the ground truths of corresponding tasks available in the dataset? Yes? No? Partially available?
* `Task Type`: Type of the task asked to the crowd. It can be either binary or multi-class question. If it is a multi-class question, we specify whether it is categorical (how many categories?), interval (range?), or ordinal (how many classes?).
* `Task Content`: Content of the task asked to the crowd (text, image, etc.), and does the content available in the dataset? (Available? Unavailable? Partially available?)
* `I don't know option`: Do the crowd workers have an "I don't know" option while completing the tasks? 
* `Time spent on the task`: Does the dataset includes any information about the time spent on the tasks?


| <sub> Dataset  </sub> | <sub> Description  </sub>  | <sub> Number of tasks </sub> | <sub> Number of workers </sub> | <sub> Number of total votes </sub>  | <sub> Ground Truth </sub>  | <sub> Task Type </sub>  | <sub> Task Content </sub>  | <sub> I don't know option </sub> | <sub> Time spent on the task </sub>  |
|---|---|---|---|---|---|---|---|---|---|
| <sub> [Blue Birds](https://github.com/welinder/cubam/tree/public/demo/bluebirds)  </sub> | <sub> The task is to identify whether the image contains a blue bird or not. The dataset contains both the individual votes and the ground truths. </sub>                                                                                                                                                                                                                                                                                                                                                                                                          |  <sub> 108 </sub>  | <sub> 39 </sub>  | <sub> 4212 </sub> | <sub> Yes </sub>| <sub> binary </sub> | <sub> image, unavailable </sub>   | <sub> No </sub>  | <sub> No  </sub>  |
| <sub> [Crowdsourced Amazon Sentiment](https://github.com/Evgeneus/screening-classification-datasets/tree/master/crowdsourced-amazon-sentiment-dataset/data)  </sub> | <sub> The task is to make sentiment analysis on Amazon product reviews. There are two predicates: "is_book", "is_negative".   </sub>                                                                                                                                                                                                                                                                                                                                                                                                                       |  <sub> 1011 </sub> | <sub> 284 </sub> | <sub> 7803  </sub> | <sub> Yes  </sub> | <sub> binary  </sub>  | <sub> text, available </sub>   | <sub> No </sub>  | <sub> Unavailable </sub> |
| <sub> [Crowdsourced loneliness-slr](https://github.com/Evgeneus/crowd-machine-collaboration-for-item-screening/tree/master/data/amt_real_data)  </sub> | <sub> Each paper is assessed by three questions: (i) Does it related to the use of technology? (ii) Does it related to older adults, and (iii)  Does it related to the intervention?  </sub>                                                                                                                                                                                                                                                                                                                                                                                                                       |  <sub> 319 </sub> | <sub> 34 </sub> | <sub> 797  </sub> | <sub> Yes  </sub> | <sub> binary </sub>  | <sub> text, unavailable </sub>   | <sub> Yes </sub>  | <sub> Unavailable </sub> |
| <sub> [HITspam-UsingCrowdflower](https://github.com/ipeirotis/Get-Another-Label/tree/master/data/HITspam-UsingCrowdflower) </sub> | <sub> The dataset contains individual worker judgments and the related ground truths about whether a HIT (from Crowdflower data) should be considered as a "spam" task. </sub>                                                                                                                                                                                                                                                                                                                                                                                |  <sub> 5380 </sub> | <sub> 153 </sub> | <sub> 42762 </sub>| <sub> Partially </sub> | <sub> binary </sub> | <sub> text, unavailable </sub> | <sub> No </sub> | <sub> Unavailable </sub> |
| <sub> [HITspam-UsingMTurk](https://github.com/ipeirotis/Get-Another-Label/tree/master/data/HITspam-UsingMTurk) </sub> | <sub> The dataset contains individual worker judgments and the related ground truths about whether a HIT (from MTurk data) should be considered as a "spam" task. </sub>                                                                                                                                                                                                                                                                                                                                                                                     |  <sub> 5840 </sub> | <sub> 135 </sub> | <sub> 28354  </sub> | <sub> Partially </sub> | <sub> binary </sub>| <sub> text, unavailable </sub> | <sub> No  </sub> | <sub> Unavailable  </sub> |
| <sub> [Recognizing Textual Entailment](https://sites.google.com/site/nlpannotations/)  </sub> | <sub> Recognizing Textual Entailment dataset contains the individual worker judgments and the related ground truths about identifying whether a given Hypothesis sentence is implied by the information in the given text. </sub>                                                                                                                                                                                                                                                                                                                           |  <sub> 800  </sub> | <sub> 164  </sub> | <sub> 8000  </sub>  | <sub> Yes   </sub> | <sub> binary  </sub> | <sub> text, available </sub> | <sub> No   </sub> | <sub> Unavailable </sub> |
| <sub> [Sentiment popularity - AMT](https://eprints.soton.ac.uk/376544/) </sub> | <sub> This dataset contains positive or negative judgments of workers for 500 sentences extracted from movie reviews, with gold labels assigned by the website. </sub>                                                                                                                                                               |  <sub> 500 </sub> | <sub> 143 </sub> |  <sub> 10000 </sub> | <sub> Yes </sub> | <sub> binary </sub>| <sub> text, unavailable </sub>  | <sub> No  </sub> | <sub> Yes  </sub> |
| <sub> [Temporal Ordering](https://sites.google.com/site/nlpannotations/)  </sub>  | <sub> Temporal Ordering dataset contains the individual worker votes and the corresponding ground truths for the task of identifying whether one event happens before another event in a given context.  </sub>                                                                                                                                                                                                                                                                                                                                            |  <sub> 462 </sub> | <sub> 76 </sub> | <sub> 4620  </sub> | <sub> Yes  </sub> | <sub> binary </sub> | <sub> text, partially available </sub>   | <sub>  No  </sub>  | <sub> Unavailable  </sub> |
| <sub> [Text Highlighting](https://figshare.com/articles/Crowdsourced_dataset_to_study_the_generation_and_impact_of_text_highlighting_in_classification_tasks/9917162)  </sub> | <sub> This dataset contains two kinds of tasks: (i) classification tasks with highlighting support, and (ii) highlighting tasks, where the workers highlight evidence.   </sub>                                                                                                                                                                                                                                                                                         |  <sub> 685 </sub> | <sub> 1851 </sub> | <sub> 27711  </sub> | <sub> Yes  </sub> | <sub> binary </sub>  | <sub> text, available </sub>   | <sub> Maybe option </sub>  | <sub> Available </sub> |
| <sub> [Toloka Aggregation Relevance 2](https://research.yandex.com/datasets/toloka) </sub> | <sub> This dataset contains approximately 0.5 million anonymized individual votes that collected in the "Relevance 2 Gradations" project in 2016. </sub>                                                                                                                                                                                                                                                                                                                                                                                                          |  <sub> 99319  </sub> | <sub> 7139  </sub> | <sub> 475536  </sub>  | <sub> Partially </sub> | <sub> binary  </sub> | <sub> text, unavailable </sub>   | <sub> No  </sub>  | <sub> Unavailable  </sub>  |
| <sub> [2010 Crowdsourced Web Relevance Judgments Data](https://www.ischool.utexas.edu/~ml/data/trec-rf10-crowd.tgz) </sub> | <sub>The dataset contains the judgments about the relevance of English Web pages from the ClueWeb09 collection (http://lemurproject.org/clueweb09/). The judgments are based on 3 scales: highly relevant, relevant, and non-relevant. A fourth judgment option indicated a broken link which could not be judged. </sub> |  <sub> 20232 </sub> | <sub> 766 </sub> | <sub> 98453 </sub> | <sub> Yes </sub>  | <sub> multi, 3 classes </sub> | <sub> text, unavailable </sub> | <sub> No  </sub> | <sub> Unavailable </sub> |
| <sub> [AdultContent2](https://github.com/ipeirotis/Get-Another-Label/tree/master/data/AdultContent2)  </sub>  | <sub>This dataset contains approximately 100K individual worker judgments and the related ground truths for classification of websites into 5 categories. </sub>  |  <sub> 11040   </sub> | <sub> 269 </sub> | <sub> 92721 </sub>  | <sub> Partially </sub>  | <sub> multi, 5 categories </sub>  | <sub> text, unavailable </sub>  | <sub> No </sub>  | <sub> Unavailable </sub>  |
| <sub> [AdultContent3](https://github.com/ipeirotis/Get-Another-Label/tree/master/data/AdultContent3-HCOMP2010) </sub> | <sub>This dataset contains approximately 50K individual worker judgments and the related ground truths for classification of websites into 4 categories. </sub>  |  <sub> 500 </sub> | <sub> 100 </sub> | <sub> 50000 </sub> | <sub> No </sub>| <sub> multi, 4 categories </sub> | <sub> text, unavailable </sub>  | <sub> No  </sub>  | <sub> Unavailable </sub>  |
| <sub> [Emotion](https://sites.google.com/site/nlpannotations/) </sub> | <sub> This dataset contains individual worker votes that rate the emotion of a given text, based on the followings: anger, disgust, fear, joy, sadness, surprise, valence. Furthermore, each rating contains a value from -100 to 100 for each emotion about the text. </sub>                                                                                                                                                                                                                                                                                     |  <sub> 700  </sub> | <sub> 10 </sub> |  <sub> 7000 </sub> | <sub> Yes </sub> | <sub> multi, interval (-100,100) </sub> | <sub> text, available </sub> | <sub> No  </sub>  | <sub> Unavailable </sub> |
| <sub> [Toloka Aggregation Relevance 5](https://research.yandex.com/datasets/toloka) </sub> | <sub> This dataset contains the judgments on the relevance of a document for a query on a 5-graded scale.  </sub>                                                                                                                                                                                                                                                                                                                                                                                                          |  <sub> 363814  </sub> | <sub> 1274 </sub> | <sub> 1091918  </sub>  | <sub> Partially </sub> | <sub> multi, 5 classes </sub> | <sub> text, unavailable   </sub>   | <sub> No  </sub>  | <sub> Unavailable  </sub>  |
| <sub> [Weather Sentiment - AMT](https://eprints.soton.ac.uk/376543/) </sub> | <sub> This dataset contains the sentiment judgments of 300 tweets. The classification task is based on the following categories: negative (0), neutral (1), positive (2), tweet not related to weather (3) and can't tell (4). </sub>                                                                                                                          |  <sub> 300 </sub> | <sub> 110 </sub> | <sub> 6000 </sub> | <sub> Yes </sub> | <sub> multi, 5 classes </sub> | <sub> text, unavailable </sub> | <sub> Yes </sub> | <sub> Yes </sub> |
| <sub> [Word Pair Similarity](https://sites.google.com/site/nlpannotations/)  </sub> | <sub> This dataset contains the individual worker votes that assign a numerical similarity score between 0 and 10 to a given text.   </sub>                                                                                                                                                                                                                                                                                                                                                                                                                       |  <sub> 30 </sub> | <sub> 10 </sub> | <sub> 300  </sub> | <sub> Yes  </sub> | <sub> multi, interval (0,10) </sub>  | <sub> text, unavailable </sub>   | <sub> No </sub>  | <sub> Unavailable </sub> |

## Download

We provide two python scripts that will help you to download all the datasets, and then transform them to a standard format. In order to do that, you should first run the `download_datasets.py`, and then `transform_datasets.py`. The required python version is 3.7, and the following modules should be installed on your system: `os, pandas, wget, zipfile, tarfile, re, platform, and shutil`.

Running the two scripts in given order will create one csv file within each dataset folder. These csv files will be in a standard format that includes the following columns, respectively: 

* `workerID`: ID of the crowd worker.
* `taskID`: ID of the task answered by the corresponding worker.
* `response`: Response of the corresponding worker on the task identified by `taskID`.
* `goldLabel`: Gold label of the corresponding task (if available).
* `taskContent`: Content of the task answered by the worker (if available).

Only `Sentiment popularity - AMT` and `Weather Sentiment - AMT` datasets will have an additional column: 

* `timeSpent`: How much time the corresponding worker spent on this task?

**P.S.** If the original dataset includes multi-predicates for a task, then we create one csv file for each predicate in the transformed version of the dataset.

(**You should not modify any of the directory names and/or dataset files you downloaded from this repo to obtain the resulting csv files accurately**)

## Usage consent

By using this tool you agree to acknowledge the original datasets and to check their terms and conditions. Some data providers may require authentication, filling forms, etc. We include a link to the original source both in the table above and in the individual repository folders for usefulness.
