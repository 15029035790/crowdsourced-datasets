# CrowdData

CrowdData is an open repository that aggregates the crowdsourced datasets that have the individual crowd votes. **We donot store the actual datasets here.** Instead, we provide the link to the original source of each dataset. In addition, we provide python scripts to automatically download all datasets and transform them to a unique format. However, the download script might not work in all cases (**some data providers may require authentication, filling forms, etc...**). That is why, **it is your responsibility to go and download the datasets in an appropriate way**. 

"text-highlighting" dataset within binary classification folder includes the datasets we collected, while all others are publicly available datasets. Each directory has a readme file that explains the details of datasets, and how to use them.

## public datasets

Table below shows an overview of the public datasets. We categorized them in two folders: `classification` and `rating`. 
Within each folder, we keep a separate folder for each dataset having a link to the original source. 

We provide two python scripts that will help you to download all the datasets, and then transform them to a unique format. In order to do that, you should first run the `download_datasets.py`, and then `transform_datasets.py`. The required python version is 3.7, and the following modules should be installed on your system: `os, pandas, wget, zipfile, tarfile, re, platform, and shutil`.

Running the two scripts in given order will create one csv file within each dataset folder. These csv files will be in a unique format that includes the following columns, respectively: `workerID`, `taskID`, `response`,
`goldLabel`, `taskContent`. Only `Sentiment popularity - AMT` and `Weather Sentiment - AMT` datasets will have an additional 
column: `timeSpent`.

(**You should not modify any of the directory names and/or dataset files you downloaded from this repo, in order to obtain the resultant csv files accurately**)

| <sub> Dataset  </sub> | <sub> Description  </sub>  | <sub> Number of questions </sub> | <sub> Number of workers </sub> | <sub> Number of total votes </sub>  | <sub> Ground Truth </sub>  | <sub> Question Type </sub>  | <sub> Question Content </sub>  | <sub> I don't know option </sub> | <sub> Time spent on task </sub>  |
|---|---|---|---|---|---|---|---|---|---|
| <sub> AdultContent2  </sub>  | <sub>This dataset contains approximately 100K individual worker judgments and the related ground truths for classification of websites into 5 categories. </sub>  |  <sub> 11040   </sub> | <sub> 269 </sub> | <sub> 92721 </sub>  | <sub> Partially </sub>  | <sub> 5-class question </sub>  | <sub> text, unavailable </sub>  | <sub> No </sub>  | <sub> Unavailable </sub>  |
| <sub> AdultContent3 </sub> | <sub>This dataset contains approximately 50K individual worker judgments and the related ground truths for classification of websites into 4 categories. </sub>  |  <sub> 500 </sub> | <sub> 100 </sub> | <sub> 50000 </sub> | <sub> No </sub>| <sub> 4-class question </sub> | <sub> text, unavailable </sub>  | <sub> No  </sub>  | <sub> Unavailable </sub>  |
| <sub> 2010 Crowdsourced Web Relevance Judgments Data </sub> | <sub>The dataset contains the judgments about the relevance of English Web pages from the ClueWeb09 collection (http://lemurproject.org/clueweb09/). The judgments are based on 3 scales: highly relevant, relevant, and non-relevant. A fourth judgment option indicated a broken link which could not be judged. </sub> |  <sub> 20232 </sub> | <sub> 766 </sub> | <sub> 98453 </sub> | <sub> Yes </sub>  | <sub> 3-class question </sub> | <sub> text, unavailable </sub> | <sub> No  </sub> | <sub> Unavailable </sub> |
| <sub> Weather Sentiment - AMT </sub> | <sub> This dataset contains the sentiment judgments of 300 tweets. The classification task is based on the following categories: negative (0), neutral (1), positive (2), tweet not related to weather (3) and can't tell (4). </sub>                                                                                                                          |  <sub> 300 </sub> | <sub> 110 </sub> | <sub> 6000 </sub> | <sub> Yes </sub> | <sub> 5-class question </sub> | <sub> text, unavailable </sub> | <sub> Yes </sub> | <sub> Yes </sub> |
| <sub> HITspam-UsingCrowdflower </sub> | <sub> The dataset contains individual worker judgments and the related ground truths about whether a HIT (from Crowdflower data) should be considered as a "spam" task. </sub>                                                                                                                                                                                                                                                                                                                                                                                |  <sub> 5380 </sub> | <sub>  </sub> | <sub> 42762 </sub>| <sub> Partially </sub> | <sub> binary question </sub> | <sub> text, unavailable </sub> | <sub> No </sub> | <sub> Unavailable </sub> |
| <sub> HITspam-UsingMTurk </sub> | <sub> The dataset contains individual worker judgments and the related ground truths about whether a HIT (from MTurk data) should be considered as a "spam" task. </sub>                                                                                                                                                                                                                                                                                                                                                                                     |  <sub> 5840 </sub> | <sub> 135 </sub> | <sub> 28354  </sub> | <sub> Partially </sub> | <sub> binary question </sub>| <sub> text, unavailable </sub> | <sub> No  </sub> | <sub> Unavailable  </sub> |
| <sub> Temporal Ordering  </sub>  | <sub> Temporal Ordering dataset contains the individual worker votes and the corresponding ground truths for the task of identifying whether one event happens before another event in a given context.  </sub>                                                                                                                                                                                                                                                                                                                                            |  <sub> 462 </sub> | <sub> 76 </sub> | <sub> 4620  </sub> | <sub> Yes  </sub> | <sub> binary question </sub> | <sub> text, partially available </sub>   | <sub>  No  </sub>  | <sub> Unavailable  </sub> |
| <sub> Recognizing Textual Entailment  </sub> | <sub> Recognizing Textual Entailment dataset contains the individual worker judgments and the related ground truths about identifying whether a given Hypothesis sentence is implied by the information in the given text. </sub>                                                                                                                                                                                                                                                                                                                           |  <sub> 800  </sub> | <sub> 164  </sub> | <sub> 8000  </sub>  | <sub> Yes   </sub> | <sub> binary question  </sub> | <sub> text, available </sub> | <sub> No   </sub> | <sub> Unavailable </sub> |
| <sub> Toloka Aggregation Relevance 2 </sub> | <sub> This dataset contains approximately 0.5 million anonymized individual votes that collected in the "Relevance 2 Gradations" project in 2016. </sub>                                                                                                                                                                                                                                                                                                                                                                                                          |  <sub> 99319  </sub> | <sub>   </sub> | <sub> 475536  </sub>  | <sub> Partially </sub> | <sub> rating, 2-class </sub> | <sub> text, unavailable </sub>   | <sub> No  </sub>  | <sub> Unavailable  </sub>  |
| <sub> Toloka Aggregation Relevance 5 </sub> | <sub> This dataset contains the judgments on the relevance of a document for a query on a 5-graded scale.  </sub>                                                                                                                                                                                                                                                                                                                                                                                                          |  <sub> 363814  </sub> | <sub>   </sub> | <sub> 1091918  </sub>  | <sub> Partially </sub> | <sub> rating, 5-class </sub> | <sub> text, unavailable   </sub>   | <sub> No  </sub>  | <sub> Unavailable  </sub>  |
| <sub> Blue Birds  </sub> | <sub> The task is to identify whether the image contains a blue bird or not. The dataset contains both the individual votes and the ground truths. </sub>                                                                                                                                                                                                                                                                                                                                                                                                          |  <sub> 108 </sub>  | <sub> 39 </sub>  | <sub> 4212 </sub> | <sub> Yes </sub>| <sub> binary question </sub> | <sub> image, unavailable </sub>   | <sub> No </sub>  | <sub> No  </sub>  |
| <sub> Sentiment popularity - AMT </sub> | <sub> This dataset contains positive or negative judgments of workers for 500 sentences extracted from movie reviews, with gold labels assigned by the website. </sub>                                                                                                                                                               |  <sub> 500 </sub> | <sub> 143 </sub> |  <sub> 10000 </sub> | <sub> Yes </sub> | <sub> binary question </sub>| <sub> text, unavailable </sub>  | <sub> No  </sub> | <sub> Yes  </sub> |
| <sub> Emotion </sub> | <sub> This dataset contains individual worker votes that rate the emotion of a given text, based on the followings: anger, disgust, fear, joy, sadness, surprise, valence. Furthermore, each rating contains a value from -100 to 100 for each emotion about the text. </sub>                                                                                                                                                                                                                                                                                     |  <sub> 700  </sub> | <sub> 10 </sub> |  <sub> 7000 </sub> | <sub> Yes </sub> | <sub> rating (-100,100) </sub> | <sub> text, available </sub> | <sub> No  </sub>  | <sub> Unavailable </sub> |
| <sub> Word Pair Similarity  </sub> | <sub> This dataset contains the individual worker votes that assign a numerical similarity score between 0 and 10 to a given text.   </sub>                                                                                                                                                                                                                                                                                                                                                                                                                       |  <sub> 30 </sub> | <sub> 10 </sub> | <sub> 300  </sub> | <sub> Yes  </sub> | <sub> rating (0,10) </sub>  | <sub> text, unavailable </sub>   | <sub> No </sub>  | <sub> Unavailable </sub> |
