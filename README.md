# CrowdData

CrowdData is an open repository that aggregates the crowdsourced datasets that have individual crowd votes. We aim at providing the available datasets with a standard format (explained in `Download` section below) so that they can be directly used in experiments, without any work-load in preprocessing. Datasets included in this repo are mainly for classification/rating tasks. CrowData can benefit researchers investigating hybrid usage of machine and human-in-the-loop in classification tasks (the repo includes some datasets having the actual content of the questions), human in classification and ranking tasks, truth discovery based on crowdsourced data, estimation of the crowd bias, and active learning.

## Datasets

We categorized the datasets in two folders: `classification` and `rating`. Within each folder, we keep a separate folder for each dataset having a link to the original source. Table below shows an overview of the public datasets (text-highlighting dataset in binary classification folder is not included in this table as we collected it). The columns of the table are as follows:

* `Dataset`: Name of the dataset including a link to the original source.
* `Description`: A brief description of the dataset.
* `Number of questions`: Basically, the number of questions asked to the crowd.
* `Number of workers`: Number of crowd workers answering the questions.
* `Number of total votes`: Number of votes collected.
* `Ground Truth`: Is the ground truth available in the dataset? Yes? No? Partially available?
* `Question Type`: Type of the question asked to the crowd.
* `Question Content`: Content of the question asked to the crowd (text, image, etc.), and does the content available in the dataset? (Available? Unavailable? Partially available?)
* `I don't know option`: Do the crowd workers have an "I don't know" option while answering the questions?
* `Time spent on the task`: Does the dataset includes any information about the time spent on the questions?


| <sub> Dataset  </sub> | <sub> Description  </sub>  | <sub> Number of questions </sub> | <sub> Number of workers </sub> | <sub> Number of total votes </sub>  | <sub> Ground Truth </sub>  | <sub> Question Type </sub>  | <sub> Question Content </sub>  | <sub> I don't know option </sub> | <sub> Time spent on the task </sub>  |
|---|---|---|---|---|---|---|---|---|---|
| <sub> [AdultContent2](https://github.com/ipeirotis/Get-Another-Label/tree/master/data/AdultContent2)  </sub>  | <sub>This dataset contains approximately 100K individual worker judgments and the related ground truths for classification of websites into 5 categories. </sub>  |  <sub> 11040   </sub> | <sub> 269 </sub> | <sub> 92721 </sub>  | <sub> Partially </sub>  | <sub> 5-class question </sub>  | <sub> text, unavailable </sub>  | <sub> No </sub>  | <sub> Unavailable </sub>  |
| <sub> [AdultContent3](https://github.com/ipeirotis/Get-Another-Label/tree/master/data/AdultContent3-HCOMP2010) </sub> | <sub>This dataset contains approximately 50K individual worker judgments and the related ground truths for classification of websites into 4 categories. </sub>  |  <sub> 500 </sub> | <sub> 100 </sub> | <sub> 50000 </sub> | <sub> No </sub>| <sub> 4-class question </sub> | <sub> text, unavailable </sub>  | <sub> No  </sub>  | <sub> Unavailable </sub>  |
| <sub> [2010 Crowdsourced Web Relevance Judgments Data](https://www.ischool.utexas.edu/~ml/data/trec-rf10-crowd.tgz) </sub> | <sub>The dataset contains the judgments about the relevance of English Web pages from the ClueWeb09 collection (http://lemurproject.org/clueweb09/). The judgments are based on 3 scales: highly relevant, relevant, and non-relevant. A fourth judgment option indicated a broken link which could not be judged. </sub> |  <sub> 20232 </sub> | <sub> 766 </sub> | <sub> 98453 </sub> | <sub> Yes </sub>  | <sub> 3-class question </sub> | <sub> text, unavailable </sub> | <sub> No  </sub> | <sub> Unavailable </sub> |
| <sub> [Weather Sentiment - AMT](https://eprints.soton.ac.uk/376543/) </sub> | <sub> This dataset contains the sentiment judgments of 300 tweets. The classification task is based on the following categories: negative (0), neutral (1), positive (2), tweet not related to weather (3) and can't tell (4). </sub>                                                                                                                          |  <sub> 300 </sub> | <sub> 110 </sub> | <sub> 6000 </sub> | <sub> Yes </sub> | <sub> 5-class question </sub> | <sub> text, unavailable </sub> | <sub> Yes </sub> | <sub> Yes </sub> |
| <sub> [HITspam-UsingCrowdflower](https://github.com/ipeirotis/Get-Another-Label/tree/master/data/HITspam-UsingCrowdflower) </sub> | <sub> The dataset contains individual worker judgments and the related ground truths about whether a HIT (from Crowdflower data) should be considered as a "spam" task. </sub>                                                                                                                                                                                                                                                                                                                                                                                |  <sub> 5380 </sub> | <sub>  </sub> | <sub> 42762 </sub>| <sub> Partially </sub> | <sub> binary question </sub> | <sub> text, unavailable </sub> | <sub> No </sub> | <sub> Unavailable </sub> |
| <sub> [HITspam-UsingMTurk](https://github.com/ipeirotis/Get-Another-Label/tree/master/data/HITspam-UsingMTurk) </sub> | <sub> The dataset contains individual worker judgments and the related ground truths about whether a HIT (from MTurk data) should be considered as a "spam" task. </sub>                                                                                                                                                                                                                                                                                                                                                                                     |  <sub> 5840 </sub> | <sub> 135 </sub> | <sub> 28354  </sub> | <sub> Partially </sub> | <sub> binary question </sub>| <sub> text, unavailable </sub> | <sub> No  </sub> | <sub> Unavailable  </sub> |
| <sub> [Temporal Ordering](https://sites.google.com/site/nlpannotations/)  </sub>  | <sub> Temporal Ordering dataset contains the individual worker votes and the corresponding ground truths for the task of identifying whether one event happens before another event in a given context.  </sub>                                                                                                                                                                                                                                                                                                                                            |  <sub> 462 </sub> | <sub> 76 </sub> | <sub> 4620  </sub> | <sub> Yes  </sub> | <sub> binary question </sub> | <sub> text, partially available </sub>   | <sub>  No  </sub>  | <sub> Unavailable  </sub> |
| <sub> [Recognizing Textual Entailment](https://sites.google.com/site/nlpannotations/)  </sub> | <sub> Recognizing Textual Entailment dataset contains the individual worker judgments and the related ground truths about identifying whether a given Hypothesis sentence is implied by the information in the given text. </sub>                                                                                                                                                                                                                                                                                                                           |  <sub> 800  </sub> | <sub> 164  </sub> | <sub> 8000  </sub>  | <sub> Yes   </sub> | <sub> binary question  </sub> | <sub> text, available </sub> | <sub> No   </sub> | <sub> Unavailable </sub> |
| <sub> [Toloka Aggregation Relevance 2](https://research.yandex.com/datasets/toloka) </sub> | <sub> This dataset contains approximately 0.5 million anonymized individual votes that collected in the "Relevance 2 Gradations" project in 2016. </sub>                                                                                                                                                                                                                                                                                                                                                                                                          |  <sub> 99319  </sub> | <sub>   </sub> | <sub> 475536  </sub>  | <sub> Partially </sub> | <sub> rating, 2-class </sub> | <sub> text, unavailable </sub>   | <sub> No  </sub>  | <sub> Unavailable  </sub>  |
| <sub> [Toloka Aggregation Relevance 5](https://research.yandex.com/datasets/toloka) </sub> | <sub> This dataset contains the judgments on the relevance of a document for a query on a 5-graded scale.  </sub>                                                                                                                                                                                                                                                                                                                                                                                                          |  <sub> 363814  </sub> | <sub>   </sub> | <sub> 1091918  </sub>  | <sub> Partially </sub> | <sub> rating, 5-class </sub> | <sub> text, unavailable   </sub>   | <sub> No  </sub>  | <sub> Unavailable  </sub>  |
| <sub> [Blue Birds](https://github.com/welinder/cubam/tree/public/demo/bluebirds)  </sub> | <sub> The task is to identify whether the image contains a blue bird or not. The dataset contains both the individual votes and the ground truths. </sub>                                                                                                                                                                                                                                                                                                                                                                                                          |  <sub> 108 </sub>  | <sub> 39 </sub>  | <sub> 4212 </sub> | <sub> Yes </sub>| <sub> binary question </sub> | <sub> image, unavailable </sub>   | <sub> No </sub>  | <sub> No  </sub>  |
| <sub> [Sentiment popularity - AMT](https://eprints.soton.ac.uk/376544/) </sub> | <sub> This dataset contains positive or negative judgments of workers for 500 sentences extracted from movie reviews, with gold labels assigned by the website. </sub>                                                                                                                                                               |  <sub> 500 </sub> | <sub> 143 </sub> |  <sub> 10000 </sub> | <sub> Yes </sub> | <sub> binary question </sub>| <sub> text, unavailable </sub>  | <sub> No  </sub> | <sub> Yes  </sub> |
| <sub> [Emotion](https://sites.google.com/site/nlpannotations/) </sub> | <sub> This dataset contains individual worker votes that rate the emotion of a given text, based on the followings: anger, disgust, fear, joy, sadness, surprise, valence. Furthermore, each rating contains a value from -100 to 100 for each emotion about the text. </sub>                                                                                                                                                                                                                                                                                     |  <sub> 700  </sub> | <sub> 10 </sub> |  <sub> 7000 </sub> | <sub> Yes </sub> | <sub> rating (-100,100) </sub> | <sub> text, available </sub> | <sub> No  </sub>  | <sub> Unavailable </sub> |
| <sub> [Word Pair Similarity](https://sites.google.com/site/nlpannotations/)  </sub> | <sub> This dataset contains the individual worker votes that assign a numerical similarity score between 0 and 10 to a given text.   </sub>                                                                                                                                                                                                                                                                                                                                                                                                                       |  <sub> 30 </sub> | <sub> 10 </sub> | <sub> 300  </sub> | <sub> Yes  </sub> | <sub> rating (0,10) </sub>  | <sub> text, unavailable </sub>   | <sub> No </sub>  | <sub> Unavailable </sub> |

## Download

We provide two python scripts that will help you to download all the datasets, and then transform them to a standard format. In order to do that, you should first run the `download_datasets.py`, and then `transform_datasets.py`. The required python version is 3.7, and the following modules should be installed on your system: `os, pandas, wget, zipfile, tarfile, re, platform, and shutil`.

Running the two scripts in given order will create one csv file within each dataset folder. These csv files will be in a standard format that includes the following columns, respectively: 

* `workerID`: ID of the crowd worker.
* `taskID`: ID of the task/question answered.
* `response`: Response of the corresponding worker on the task identified by `taskID`.
* `goldLabel`: Gold label of the corresponding task (if available).
* `taskContent`: Content of the task/question answered by the worker (if available).

Only `Sentiment popularity - AMT` and `Weather Sentiment - AMT` datasets will have an additional column: 

* `timeSpent`: How much time the corresponding worker spent on this task?

(**You should not modify any of the directory names and/or dataset files you downloaded from this repo, in order to obtain the resultant csv files accurately**)

## Usage consent

By using this tool you agree to acknowledge the original datasets and to check their terms and conditions. Some data providers may require authentication, filling forms, etc. We include a link to the original source both in the table above and in the individual repository folders for usefulness.
