# 2010 Crowdsourced Web Relevance Judgments

Link to the original source: https://www.ischool.utexas.edu/~ml/data/trec-rf10-crowd.tgz 

The dataset contains the judgments of 766 anonymized AMT workers about the relevance of English Web pages from the ClueWeb09 collection (http://lemurproject.org/clueweb09/) for English search queries drawn from the TREC 2009 Million Query track (http://ir.cis.udel.edu/million). The judgments are based on 3 scales: highly relevant, relevant, and non-relevant. A fourth judgment option indicated a broken link which could not be judged. 98,453 judgments produced by the workers, 3277 of which have the ground truths provided by NIST. From the source given above, `trec-rf10-data.txt` file should be copied into the `data-raw` folder.

**Cite this work as:**
```
@inproceedings{Buckley10-notebook,
  author={Chris Buckley and Matthew Lease and Mark D. Smucker},
  title={{Overview of the TREC 2010 Relevance Feedback Track (Notebook)}},
  booktitle={{The Nineteenth Text Retrieval Conference (TREC) Notebook}},
  institute = {{National Institute of Standards and Technology (NIST)}},
  year={2010}
}
```




