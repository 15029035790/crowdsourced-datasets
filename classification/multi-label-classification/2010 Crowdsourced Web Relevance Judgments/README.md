# 2010 Crowdsourced Web Relevance Judgments

Link to the original source: https://www.ischool.utexas.edu/~ml/data/trec-rf10-crowd.tgz (Chris Buckley, Matthew Lease and Mark D. Smucker. Overview of the TREC 2010 Relevance Feedback Track (Notebook). NIST, 2010.)

The dataset contains the judgments of 766 anonymized AMT workers about the relevance of English Web pages from the ClueWeb09 collection (http://lemurproject.org/clueweb09/) for English search queries drawn from the TREC 2009 Million Query track (http://ir.cis.udel.edu/million). The judgments are based on 3 scales: highly relevant, relevant, and non-relevant. A fourth judgment option indicated a broken link which could not be judged. 98,453 judgments produced by the workers, 3277 of which have the ground truths provided by NIST. From the source given above, `trec-rf10-data.txt` file should be copied into the `data-raw` folder.






